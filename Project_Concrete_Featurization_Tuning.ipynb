{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Description:\n",
    "##### The actual concrete compressive strength (MPa) for a given mixture under a specific age (days) was determined from laboratory. Data is in raw form (not scaled). The data has 8 quantitative input variables, and 1 quantitative output variable, and 1030 instances (observations).\n",
    "\n",
    "### Domain:\n",
    "##### Cement manufacturing\n",
    "\n",
    "### Context:\n",
    "##### Concrete is the most important material in civil engineering. The concrete compressive strength is a highly nonlinear function of age and ingredients. These ingredients include cement, blast furnace slag, fly ash, water, superplasticizer, coarse aggregate, and fine aggregate.\n",
    "\n",
    "### Attribute Information:\n",
    "- Cement : measured in kg in a m3 mixture\n",
    "- Blast : measured in kg in a m3 mixture\n",
    "- Fly ash : measured in kg in a m3 mixture\n",
    "- Water : measured in kg in a m3 mixture\n",
    "- Superplasticizer : measured in kg in a m3 mixture\n",
    "- Coarse Aggregate : measured in kg in a m3 mixture\n",
    "- Fine Aggregate : measured in kg in a m3 mixture\n",
    "- Age : day (1~365)\n",
    "- Concrete compressive strength measured in MPa\n",
    "\n",
    "### Learning Outcomes:\n",
    "- Exploratory Data Analysis\n",
    "- Building ML models for regression\n",
    "- Hyper parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "#%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from scipy.stats import zscore\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import Linear Regression machine learning library\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Import KNN Regressor machine learning library\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "# Import Decision Tree Regressor machine learning library\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "# Import ensemble machine learning library\n",
    "from sklearn.ensemble import (RandomForestRegressor, GradientBoostingRegressor,AdaBoostRegressor,BaggingRegressor)\n",
    "# Import support vector regressor machine learning library\n",
    "from sklearn.svm import SVR\n",
    "#Import the metrics\n",
    "from sklearn import metrics\n",
    "#Import the Voting regressor for Ensemble\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "# Import stats from scipy\n",
    "from scipy import stats\n",
    "#importing the metrics\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score\n",
    "#importing the K fold\n",
    "from sklearn.model_selection import KFold\n",
    "#importing the cross validation score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "#importing the preprocessing library\n",
    "from sklearn import preprocessing\n",
    "# importing the Polynomial features\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "#importing kmeans clustering library\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.utils import resample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "df=pd.read_csv(\"concrete (1).csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Types and Data set values Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get info on data avialable and compare it with details provided about data set\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Total 1030 rows and 9 columns present\n",
    "- All data are numerical as expected from the description\n",
    "- As all values are numeric then there is less chances of data having text or symbols apart from few unexpected numbers if any as outliers\n",
    "- Non- NULL count is same for all column suggests no null values\n",
    "- We have 8 independent and 1 dependent values\n",
    "- Target Column: -Strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Analyze distribution\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All columns are qantitative\n",
    "- There is huge gap between min and max values for columns\n",
    "- Mean and 50% are not similar for many cloumns, represents skewness for those data.\n",
    "- Mean is higher for slag, ash,age compared to 50%, represents right skewness (long tail towards right) \n",
    "- many min values are at 0, need to validate its feasibility and practicality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view the top 5 rows of data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view bottom 5 rows of data\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view random 10 data from dataset\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With head, tail and random samples we can notice there is a diverse spread of data over all attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows = 9, ncols=2, figsize = (10,30))\n",
    "\n",
    "\n",
    "for i, x in enumerate(df.columns):\n",
    "    sns.distplot(df[x],ax=axs[i,0],bins=50, rug=True)\n",
    "    sns.boxplot(orient='v', data=df[x], ax=axs[i, 1])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.clf()\n",
    "plt.close()\n",
    "#for colname in df.columns:\n",
    "#    sns.distplot(df[colname])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cement: has close to normal distribution (slight tail towards higher denominations of numbers) and no visual outliers present, mode seems to be around 200-250\n",
    "- Slag: mode of the distribution is clearly 0, rest of the dataset seems like right skewed as it has few higer numbers beyond 300. Few outliers alos visible beyond 350.\n",
    "- Ash: Similar to Slag, this also has a mode of 0, but inlike Slag it has a huge gap of values between 0 and 50 or 100. This suggests the value 0 might need a little more investigation to deicde if its a missing value or something else. No visible outliers noticed.\n",
    "- Water: ditributions has multiple peaks but its look spreaded across both side of peak. Few outliers present in both lower and higher side of values\n",
    "- Superplastic: mode is zero and its right skewed. few outlier present in higher side of values\n",
    "- Coarseagg: datapoints looks distributed and no outlier presence\n",
    "- Fineagg: Distribution seems normal but has few outlirers on higher values\n",
    "- Age: highly right skwed data points, lot of outlier presence\n",
    "- Strength: Data points seems distributed normally but few outlier presence towards higher values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.boxplot(figsize=(15, 10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From box plot we can clearly see there is high presence of outliers on slag, water, superplastic, fineagg, age and strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.pairplot(df, diag_kind='kde');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From intital view its difficult to see any strong liner or polynomial correlation among the attributes. We can observe ther are few week correlation present between few attributes. Crrelation heatmap shold provide more insights.\n",
    "- Majority of bi-variate graphs shows a cloud like structure\n",
    "- From diagonal analysis on kde plots, we can observe there is presence of minimum two distinct peaks in the independent attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.hist(bins=30,figsize=(25, 10));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(df.corr(),annot=True,cmap=\"cividis\",linecolor=\"black\" );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()['strength'].sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From heat map we can clearly observe the linear relations.\n",
    "- Majority of attributes has week correlations ranging from -3 to +3.\n",
    "- Few strong -ve correlation are visible among attributes like between: superplastic & water, superplastic & ash, fineagg & water\n",
    "- from correlation details with strength we can notice highest positive/negative correlation is with cement, superplastic, age and water.\n",
    "- We can plan for removal of low correlated attributes down the line based on analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.lmplot(x=\"cement\",y=\"strength\",data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we can notice a clear positive correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#cement vs water\n",
    "sns.lmplot(x=\"cement\",y=\"water\",data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We cal clearly see the line is almost parallel to x axis, representing very minimal correlation between the attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.lmplot(x=\"superplastic\",y=\"water\",data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we can notice the strong negative correlation line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.lmplot(x=\"ash\",y=\"strength\",data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we can notice the line is almost paralle to the x axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- No null values present"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Outliers and treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.boxplot(figsize=(10, 10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From box plot we can clearly see there is high presence of outliers on slag, water, superplastic, fineagg, age and strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUmber of outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cols in df.columns[:-1]:\n",
    "    q1=df[cols].quantile(0.25)\n",
    "    q3=df[cols].quantile(0.75)\n",
    "    iqr=q3-q1\n",
    "    \n",
    "    low=q1-1.5*iqr\n",
    "    high=q3+1.5*iqr\n",
    "    print('Outliers count for',cols, df.loc[((df[cols]>high) | (df[cols]<low)),cols].count())\n",
    "    print('high value Outliers count for',cols, df.loc[(df[cols]>high),cols].count())\n",
    "    print('low value Outliers count for',cols, df.loc[(df[cols]<low),cols].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- there is significant number of outliers presnt in age attribute compared to other attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Records containing outliers\n",
    "for cols in df.columns[:-1]:\n",
    "    q1=df[cols].quantile(0.25)\n",
    "    q3=df[cols].quantile(0.75)\n",
    "    iqr=q3-q1\n",
    "    \n",
    "    low=q1-1.5*iqr\n",
    "    high=q3+1.5*iqr\n",
    "    print()\n",
    "    print('Outliers count for',cols)\n",
    "    print(df.loc[((df[cols]>high) | (df[cols]<low)),:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we will be treating the columns with same logic used in box plot using IQR\n",
    "- We are replacing the Outliers with boundry values to keep the higer values colse to higher and lower boundries.(Capping)\n",
    "- Goal is to keep the value inside acceptable ranges without loosing its main charatcteristic of being outlier or being a higher/lower number. Using this approach we will slightly increase the frequency on boundry values but they will be inside range. We have to keep an eye ot for a bigger group formation towards the boundry or a peak in graphs.\n",
    "- Replacing with Mean or median might miss represent the main characteristics of the data set which is pushing it to be be outlier.(Introducing bias by making it a normal curve)\n",
    "- We will be doing this only once, we may see new outliers as the IQR might change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- selecting all but leaving out the last columns whihc is our target\n",
    "for cols in df.columns[:-1]:\n",
    "    q1=df[cols].quantile(0.25)\n",
    "    q3=df[cols].quantile(0.75)\n",
    "    iqr=q3-q1\n",
    "    \n",
    "    low=q1-1.5*iqr\n",
    "    high=q3+1.5*iqr\n",
    "    \n",
    "    df.loc[(df[cols]<low),cols]=low\n",
    "    df.loc[(df[cols]>high),cols]=high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Rechecking the outliers and qartiles\n",
    "df.boxplot(figsize=(10, 8));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- No more outliers present in the independent columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to capture if we are getting any additional peaks or bulges on the due to outlier treatments\n",
    "sns.pairplot(df, diag_kind='kde');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Peaks are almost similar to what we have noticed earlier which suggestes our outlier treatments has not affected highly towards the borders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering, Model Building and Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All the feature showed less correlation internally, \n",
    "- few features have very less correlation with Strength as well which we can identify and eliminate\n",
    "- we will be using lasso to identify which all features have 0 contributing factor and \n",
    "- compare with what we have analyzed during correlation heatmap analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling data set\n",
    "\n",
    "\n",
    "df_scaled = preprocessing.scale(df)\n",
    "df_scaled=pd.DataFrame(df_scaled,columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_scaled.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spliting dependent and independent variable\n",
    "\n",
    "X=df_scaled.iloc[:,0:8]\n",
    "y = df_scaled.iloc[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and test set in 70:30 ratio\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS - LinearRegression\n",
    "# We have noticed earlier that all the features were not very strong predictors of strength (max corr was 66% with cement)\n",
    "# we are expecting our Linear model to be a weak predictor\n",
    "regression_model = LinearRegression()\n",
    "regression_model.fit(X_train, y_train)\n",
    "\n",
    "for idx, col_name in enumerate(X_train.columns):\n",
    "    print(\"The coefficient for {} is {}\".format(col_name, regression_model.coef_[idx]))\n",
    "    \n",
    "intercept = regression_model.intercept_\n",
    "\n",
    "print(\"\\nThe intercept for our model is {}\".format(intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# predict mileage (mpg) for a set of attributes not in the training or test set\n",
    "y_pred = regression_model.predict(X_test)\n",
    "\n",
    "# Since this is regression, plot the predicted y value vs actual y values for the test data\n",
    "# A good model's prediction will be close to actual leading to high R and R2 values\n",
    "plt.scatter(y_test, y_pred);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can notice the actual and predicted values are spread widely and not concentrated towards the diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ridge\n",
    "ridge = Ridge(alpha=.3)\n",
    "ridge.fit(X_train,y_train)\n",
    "#print (\"Ridge model:\", (ridge.coef_))\n",
    "\n",
    "for idx, col_name in enumerate(X_train.columns):\n",
    "    print(\"The coefficient for {} is {}\".format(col_name, ridge.coef_[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lasso\n",
    "\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X_train,y_train)\n",
    "#print (\"Lasso model:\", (lasso.coef_))\n",
    "featureAnalysisLasso=lasso.coef_\n",
    "for idx, col_name in enumerate(X_train.columns):\n",
    "    print(\"The coefficient for {} is {}\".format(col_name, featureAnalysisLasso[idx]))\n",
    "    \n",
    "#we will use the lasso coeff details later as well for feature selecction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparing all linear regression details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(regression_model.score(X_train, y_train))\n",
    "print(regression_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ridge.score(X_train, y_train))\n",
    "print(ridge.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lasso.score(X_train, y_train))\n",
    "print(lasso.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As expected from linear correlation details, OLS(linear regression) is not a strong predictor and able provide 71.5% accuracy with Train and 75.2% with test set.\n",
    "- With Ridge we can notice the results are very close to OLS\n",
    "- With Lasso we can notice a significant drop in accuracy as it has dropped 3 features from its model consideration (fineagg, coarseagg, ash)\n",
    "- we will try with polynomial approach to see if there is a significant cahnge in it.\n",
    "- From the graph analysis we haven't noticed any variable which shows a polynomial relation as well but we will git it a try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Polynomial approach and complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "#going with degree 2 only\n",
    "poly = PolynomialFeatures(degree = 2, interaction_only=True)\n",
    "\n",
    "#poly = PolynomialFeatures(2)\n",
    "#X is already scaled and will use it\n",
    "X_poly = poly.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.30, random_state=50)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Shape of train data set changed to 721, 37\n",
    "- we have 37 cloumns now instead of 8 (close to 5 fold increase in variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "regression_model.fit(X_train, y_train)\n",
    "print(regression_model.coef_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(regression_model.score(X_train, y_train))\n",
    "print(regression_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge(alpha=.3)\n",
    "ridge.fit(X_train,y_train)\n",
    "print (\"Ridge model:\", (ridge.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ridge.score(X_train, y_train))\n",
    "print(ridge.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso(alpha=0.01)\n",
    "lasso.fit(X_train,y_train)\n",
    "print (\"Lasso model:\", (lasso.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lasso.score(X_train, y_train))\n",
    "print(lasso.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Comparing the results of polynomial features with linear, ridge and Lasso, we can notice there is significant increase in the accuracy for the traina nd test data set.\n",
    "- For Lasso we can notice a significant jump in accuracy from 63%  to 78% (eliminating 12 out of 37 features).\n",
    "- Polynomial features are able to dig out relations which were not visible in the data analysis phase but we also observed multi fold increase in features which also increases the complexity of the model. We may see more improvement by increaseing the degree of the polynomial features but the complexity will be too high then. We are ending our analysis with polynomial features here and will try out few other models to see if we are able to get better numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Explore for gaussians. If data is likely to be a mix of gaussians, explore individual clusters and present your findings in terms of the independent attributes and their suitability to predict strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_scaled,diag_kind='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can clearly notice there are minimum two gausian or clear peaks visible in the data sets (slag, ash, superplastic, age). WE max observe max of 3 peaks for Age but its comapratively smaller then others.\n",
    "- Only field which has clear linear relationship with strength is cement, for all other feture we can see the distibution is spread across strength on y axis for any given point of feature value in x.\n",
    "- even in the cloud formation in the graphs, we can observe there is two clear distinct groups froming one which is close to zero and rest others\n",
    "- Based on above pairplot and below correlation details with strength (dependent) column\n",
    "- Cement:\n",
    "-- Cement seems to have clear and maximum positive correlation, data distribution looks close to normal. This feature should be included in the model preparation.\n",
    "- Age:\n",
    "-- For age we can clearly notice formation of groups for strength at diff age levels and is a positive correlation. Seems to have multipe gaussians present.This feature should be included in the model preparation.\n",
    "- superplastic:\n",
    "-- superplastic seems to have positive correlation but comaparatively less than cement or age from graphs, seems to have two gaussian or groups or peaks present from the graphs analysis. This feature should be included in the model preparation.\n",
    "- slag:\n",
    "-- slag seem to have a weak relation with strenght. There is clear indication of two gaussian. While comparing with strength we can notice there is two group formation one less than 0 and other above it. This has some significance but this feature is on the list for elimination. We will finalize the details based on further analysis.\n",
    "- ash:\n",
    "-- This feature has clearly weak correlation with strength. Two gaussinas are clealry visible. On the scatter plot with strength we can clearly see two distinct cloud formations but both doesnt have a clear head or tail. This is feature will not be beneficial in the model preparation.\n",
    "- coarseagg,fineagg:\n",
    "-- both these features have similar data distribution and correlation values. From Correlation details with strength we can notice they have negative correlation and are better than ash but visually its very hard to identify any head or tail for it. This is feature will not be much beneficial in the model preparation but will keep it for further analysis as they have higher correlation value than ash and slag.\n",
    "- water:\n",
    "-- water has high negative correlation comapred to other features having negative correlation. We can notice 3 gaussians in the distribution. On the scatter plot with strength we can notice thre group formation on less than -2, more than +2 and in between -2 to +2. This feature should be included in the model preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled.corr().strength.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to see how many groups we can capture with kmeans & GaussianMixture models\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "cluster_range=range(1,15)\n",
    "cluster_error=[]\n",
    "\n",
    "for num_clusters in cluster_range:\n",
    "    cluster=KMeans(num_clusters,n_init=20)\n",
    "    cluster.fit(temp)\n",
    "    labels=cluster.labels_\n",
    "    centroid=cluster.cluster_centers_\n",
    "    cluster_error.append(cluster.inertia_)\n",
    "\n",
    "clusters_df = pd.DataFrame({\"num_clusters\": cluster_range, \"cluster_errors\": cluster_error})\n",
    "clusters_df[0:15]\n",
    "\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(clusters_df.num_clusters, clusters_df.cluster_errors, marker = \"X\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Kmeans shows a weak prediction (a curve), which suggests its not able to identify groups in data sets clearly\n",
    "- but from kmeans clustering we can see the elbow formation at 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "temp=deepcopy(df_scaled.drop(columns=['strength'], axis=1))\n",
    "temp2=deepcopy(df_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data in 2 set\n",
    "#pandas will cut the data in 2 ranges and convert the continuous data to categorical for this purpose\n",
    "val2=pd.cut(temp2.strength, bins=2, labels=np.arange(2), right=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training gaussian mixture model \n",
    "from sklearn.mixture import GaussianMixture\n",
    "gmm = GaussianMixture(n_components=2, random_state=50)\n",
    "gmm.fit(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions from gmm\n",
    "labels = gmm.predict(temp)\n",
    "frame = pd.DataFrame(temp)\n",
    "frame['cluster'] = labels\n",
    "frame['stregth_cluster']=val2\n",
    "frame.columns = ['cement', 'slag', 'ash', 'water', 'superplastic', 'coarseagg',\n",
    "       'fineagg', 'age', 'cluster','stregth_cluster']\n",
    "\n",
    "color=['blue','green','cyan', 'black','red']\n",
    "for k in range(0,2):\n",
    "    data = frame[frame[\"cluster\"]==k]\n",
    "    plt.scatter(data[\"cement\"],data[\"water\"],c=color[k])\n",
    "plt.show()\n",
    "#plotting the clusters to identify if cluters created from  modela nd clusters created in data are any match\n",
    "#plot with split as per model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(0,2):\n",
    "    data = frame[frame[\"stregth_cluster\"]==k]\n",
    "    plt.scatter(data[\"cement\"],data[\"water\"],c=color[k])\n",
    "plt.show()\n",
    "#Plot with real split with strength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we can see there is a significant similarity on identifying the dots towards far right by model but with the cloud we can see a mix of all data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = pd.crosstab(frame['cluster'], frame['stregth_cluster'])\n",
    "display(ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From cross tab we can see there is a clear mismatch in groups and there are significant laps in recall and preceission.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance and feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature selection\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as sfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Lin Reg  to use in feature selection\n",
    "linR = LinearRegression()\n",
    "\n",
    "# Training and test set in 70:30 ratio\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 50)\n",
    "\n",
    "# Build step forward feature selection\n",
    "# we have choosen 5 as from our earlier exploration with lasso and feature analysis\n",
    "sfs1 = sfs(linR, k_features=5, forward=True, scoring='r2', cv=5)\n",
    "\n",
    "# Perform SFFS\n",
    "sfs1 = sfs1.fit(X_train.values, y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs1.get_metric_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plot_sfs(sfs1.get_metric_dict())\n",
    "\n",
    "plt.title('Sequential Forward Selection (w. R^2)')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which features?\n",
    "columnList = list(X_train.columns)\n",
    "feat_cols = list(sfs1.k_feature_idx_)\n",
    "print(feat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsetColumnList = [columnList[i] for i in feat_cols] \n",
    "print(subsetColumnList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Above is the list of features for model selection based on importance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lasso details for feature \n",
    "for idx, col_name in enumerate(X_train.columns):\n",
    "    print(\"The coefficient for {} is {}\".format(col_name, featureAnalysisLasso[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From details from Lasso feature selection, SequentialFeatureSelector and correlation Heat map we can clearly identify the 5 features we can go with for further model building and testing without loosing too much data.\n",
    "- Features identifed for Model - 'cement', 'slag', 'water', 'superplastic', 'age'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split X and y into training and test set in 70:30 ratio\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 50)\n",
    "dt_model = DecisionTreeRegressor()\n",
    "dt_model.fit(X_train , y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing the feature importance\n",
    "print('Feature importances: \\n',pd.DataFrame(dt_model.feature_importances_,columns=['Imp'],index=X_train.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From decission tree regressor as well we can notice we are ending with same features, which are cement,slag,water,superplastic, age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- During Linear correlation analysis, we have noticed there is very less attributes in data set which have high correlation or pridicting power for target attribute.\n",
    "- During our linear and polynomial analysis we have noticed the accuracy are not too high. \n",
    "- Polynomial feature were able to provide slightly improved results compared to linear but with high number of attributes.\n",
    "- Support vector regressor or KNN are strong models for classification, they have significant capabilities for regression problems as welll but doesnt seems like a suitable candiate for this regression predictions.\n",
    "- Considering above details, we will put our model building efforts on ensamble methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We are proceeding with those attributes only which have significant contribution on targets predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelComp=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X=df_scaled.iloc[:,0:8]\n",
    "y = df_scaled.iloc[:,8]\n",
    "seed=50\n",
    "num_folds = 50\n",
    "#Removing less contributing features\n",
    "X=X.drop(['ash','coarseagg','fineagg'],axis=1)\n",
    "\n",
    "# Split X and y into training and test set in 70:30 ratio\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decission tree Regreesor with default parameters and defaul values\n",
    "dt_model = DecisionTreeRegressor()\n",
    "dt_model.fit(X_train , y_train)\n",
    "\n",
    "y_pred = dt_model.predict(X_test)\n",
    "# Train data accuracy\n",
    "print('Performance on training data using DT:',dt_model.score(X_train,y_train))\n",
    "# test data accuracy\n",
    "print('Performance on testing data using DT:',dt_model.score(X_test,y_test))\n",
    "#r2 score\n",
    "acc_DT=metrics.r2_score(y_test, y_pred)\n",
    "print('Accuracy Test: ',acc_DT)\n",
    "print('MSE: ',metrics.mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr  \n",
    "sns.jointplot(x=y_test, y=y_pred, stat_func=pearsonr,kind=\"reg\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For decission tree with default sets and we can see that training set accuracy is close 99.5% but test set accuracy is 84.7%. This variation suggests we have an overfitting model.\n",
    "- with pearsonr plotting as well we can see the values are spread far from the expected line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
    "#kfold = KFold(n_splits=num_folds)\n",
    "model1 = dt_model\n",
    "results = cross_val_score(model1, X_train, y_train, cv=kfold)\n",
    "print(results)\n",
    "print(\"\\n Average model Accuracy: %.3f%% with std. dev - (%.3f%%)\" % (results.mean()*100.0, results.std()*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This model with default set is not a suitable model as it has 77% average accuracy but having a 23% variance, from detils we can notice it predicts values as low as 60% and for few data sets its around 95%. This suggests the model is overfitted and will trade poorely with many data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prunning Decission tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regularizing Decission Tree with diff values\n",
    "model = DecisionTreeRegressor( max_depth = 8,random_state=seed,min_samples_leaf=4)\n",
    "model.fit(X_train , y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "# Train data accuracy\n",
    "print('Performance on training data using DT:',model.score(X_train,y_train))\n",
    "# test data accuracy\n",
    "print('Performance on testing data using DT:',model.score(X_test,y_test))\n",
    "#r2 score\n",
    "acc_DT=metrics.r2_score(y_test, y_pred)\n",
    "print('Accuracy Test: ',acc_DT)\n",
    "print('MSE: ',metrics.mean_squared_error(y_test, y_pred))\n",
    "\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
    "#kfold = KFold(n_splits=num_folds)\n",
    "#model1 = model\n",
    "results = cross_val_score(model, X_train, y_train, cv=kfold)\n",
    "print(results)\n",
    "print(\"\\n Average model Accuracy: %.3f%% with std. dev - (%.3f%%)\" % (results.mean()*100.0, results.std()*100.0))\n",
    "\n",
    "\n",
    "modelComp=modelComp.append(pd.DataFrame({'Model':['Decission Tree'],\n",
    "                                         'Train Accuracy':[model.score(X_train,y_train)],\n",
    "                                         'Test Accuracy':[model.score(X_test , y_test)],\n",
    "                                         'Kfold-Mean-Accuracy':[results.mean()],\n",
    "                                         'Kfold-StdDeviation':[results.std()]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With Prunned decission tree we can see the Train and test data sets have close by accuracies. This reduces our Overfitting\n",
    "- But considering the cross val score and details - we can notice it still has very high std. deviation. But there is still a high gap between the train and test accuracies and we are still ending up with overfitting model. Any further prunning of the tables leads to reduced test and cross val accuracies.\n",
    "- We will move ahead with some other models and will try for Grid search on the models which have better performances in next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random FOrest With default config\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor()\n",
    "\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "# Train data accuracy\n",
    "print('Performance on training data using DT:',model.score(X_train,y_train))\n",
    "# test data accuracy\n",
    "print('Performance on testing data using DT:',model.score(X_test,y_test))\n",
    "#r2 score\n",
    "acc_DT=metrics.r2_score(y_test, y_pred)\n",
    "print('Accuracy Test: ',acc_DT)\n",
    "print('MSE: ',metrics.mean_squared_error(y_test, y_pred))\n",
    "\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
    "#kfold = KFold(n_splits=num_folds)\n",
    "#model1 = model\n",
    "results = cross_val_score(model, X_train, y_train, cv=kfold)\n",
    "print(results)\n",
    "print(\"\\n Average model Accuracy: %.3f%% with std. dev - (%.3f%%)\" % (results.mean()*100.0, results.std()*100.0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With default values only we are able to get a significant better value then Decission tree\n",
    "- Train Data set is close to 98% accuracy which hints towards overfit but test data set is as well around 89.8%, which is not very far from train accuracy\n",
    "- Model seems slightly over fit but we can give it a try with grid search to see if its make the result any better and try to close the gap between train and test accuracies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Grid search on Random Forest as its default is better than Decission tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'bootstrap': [True],\n",
    " 'max_depth': [5, 10, 15, 20, 25, 30, 35, 40, 50],\n",
    " 'max_features': ['auto', 'sqrt'],\n",
    " 'min_samples_leaf': [1, 2, 4, 8],\n",
    " 'n_estimators': [100]}\n",
    "\n",
    "\n",
    "clf = GridSearchCV(RandomForestRegressor(), \n",
    "                   parameters, \n",
    "                   cv = 5, \n",
    "                   verbose = 2, \n",
    "                   n_jobs= 4)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the grid search is giving us better train data accuracy but its leading it to overfit zone\n",
    "#so prunned the max depth after few iterations\n",
    "model = RandomForestRegressor(bootstrap = True,\n",
    " max_depth= 9,\n",
    " max_features= 'sqrt',\n",
    " min_samples_leaf= 1,\n",
    " n_estimators= 100)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "# Train data accuracy\n",
    "print('Performance on training data using DT:',model.score(X_train,y_train))\n",
    "# test data accuracy\n",
    "print('Performance on testing data using DT:',model.score(X_test,y_test))\n",
    "#r2 score\n",
    "acc_DT=metrics.r2_score(y_test, y_pred)\n",
    "print('Accuracy Test: ',acc_DT)\n",
    "print('MSE: ',metrics.mean_squared_error(y_test, y_pred))\n",
    "\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
    "#kfold = KFold(n_splits=num_folds)\n",
    "#model1 = model\n",
    "results = cross_val_score(model, X_train, y_train, cv=kfold)\n",
    "print(results)\n",
    "print(\"\\n Average model Accuracy: %.3f%% with std. dev - (%.3f%%)\" % (results.mean()*100.0, results.std()*100.0))\n",
    "\n",
    "modelComp=modelComp.append(pd.DataFrame({'Model':['Random Forest Regressor'],\n",
    "                                         'Train Accuracy':[model.score(X_train,y_train)],\n",
    "                                         'Test Accuracy':[model.score(X_test , y_test)],\n",
    "                                         'Kfold-Mean-Accuracy':[results.mean()],\n",
    "                                         'Kfold-StdDeviation':[results.std()]}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelComp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempting Random Forest with PCA to see the impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df_scaled.iloc[:,0:8]\n",
    "y = df_scaled.iloc[:,8]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = seed)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(8)# Initialize PCA object\n",
    "\n",
    "pca.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=6)\n",
    "pca.fit(X_train)\n",
    "X_train_pca = pca.transform(X_train)  # PCs for the train data\n",
    "X_test_pca = pca.transform(X_test)    # PCs for the test data\n",
    "\n",
    "X_train_pca.shape, X_test_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(bootstrap = True,\n",
    " max_depth= 15,\n",
    " max_features= 'sqrt',\n",
    " min_samples_leaf= 1,\n",
    " n_estimators= 100)\n",
    "\n",
    "rf.fit(X_train_pca, y_train)\n",
    "\n",
    "y_pred = rf.predict(X_test_pca)\n",
    "# Train data accuracy\n",
    "print('Performance on training data using DT:',rf.score(X_train_pca,y_train))\n",
    "# test data accuracy\n",
    "print('Performance on testing data using DT:',rf.score(X_test_pca,y_test))\n",
    "#r2 score\n",
    "acc_DT=metrics.r2_score(y_test, y_pred)\n",
    "print('Accuracy Test: ',acc_DT)\n",
    "print('MSE: ',metrics.mean_squared_error(y_test, y_pred))\n",
    "\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
    "#kfold = KFold(n_splits=num_folds)\n",
    "model1 = rf\n",
    "results = cross_val_score(model1, X_train, y_train, cv=kfold)\n",
    "print(results)\n",
    "print(\"\\n Average model Accuracy: %.3f%% with std. dev - (%.3f%%)\" % (results.mean()*100.0, results.std()*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With PCA implemetation and our existing data set (with selective attributes), we can see similar impact\n",
    "- we will proceed with existing data set and not with PCA implemented data set for better understanding of relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df_scaled.iloc[:,0:8]\n",
    "y = df_scaled.iloc[:,8]\n",
    "\n",
    "#Removing less contributing features\n",
    "X=X.drop(['ash','coarseagg','fineagg'],axis=1)\n",
    "\n",
    "# Split X and y into training and test set in 70:30 ratio\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=GradientBoostingRegressor()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred = model.predict(X_test)\n",
    "# Train data accuracy\n",
    "print('Performance on training data using DT:',model.score(X_train,y_train))\n",
    "# test data accuracy\n",
    "print('Performance on testing data using DT:',model.score(X_test,y_test))\n",
    "#r2 score\n",
    "acc_DT=metrics.r2_score(y_test, y_pred)\n",
    "print('Accuracy Test: ',acc_DT)\n",
    "print('MSE: ',metrics.mean_squared_error(y_test, y_pred))\n",
    "\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
    "#kfold = KFold(n_splits=num_folds)\n",
    "model1 = model\n",
    "results = cross_val_score(model1, X_train, y_train, cv=kfold)\n",
    "print(results)\n",
    "print(\"\\n Average model Accuracy: %.3f%% with std. dev - (%.3f%%)\" % (results.mean()*100.0, results.std()*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With Gradiant boost regressor we can notice the train and test set accuracies are close by compared to other models with default setting itself\n",
    "- We will try to get better close by numbers for train and Test to make it more generalized with out loosing much of test accuracies.\n",
    "- Cross val score is also better at accuracy of 85%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### we can take the following approach:\n",
    "\n",
    "- Choose a relatively high learning rate. Generally the default value of 0.1 works but somewhere between 0.05 to 0.2 should work for different problems\n",
    "- Determine the optimum number of trees for this learning rate. This should range around 5-10 as we noticed earlier dring decission tree as well. Remember to choose a value on which your system can work fairly fast. This is because it will be used for testing various scenarios and determining the tree parameters.\n",
    "- Tune tree-specific parameters for decided learning rate and number of trees. Note that we can choose different parameters to define a tree .\n",
    "- Lower the learning rate and increase the estimators proportionally to get more robust models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"loss\":['ls', 'lad', 'huber', 'quantile'],\n",
    "    \"learning_rate\": [0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2],\n",
    "    #\"min_samples_split\": np.linspace(0.1, 0.5, 5),\n",
    "    #\"min_samples_leaf\": np.linspace(0.1, 0.5, 5),\n",
    "    \"max_depth\":[3,5,8],\n",
    "    \"max_features\":[\"log2\",\"sqrt\"],\n",
    "    \"criterion\": [\"friedman_mse\",  \"mae\"],\n",
    "    \"subsample\":[0.5, 0.618, 0.8, 0.85, 0.9, 0.95, 1.0],\n",
    "    \"n_estimators\":[10]\n",
    "    }\n",
    "clf = GridSearchCV(estimator = GradientBoostingRegressor(), \n",
    "                   param_grid = parameters, \n",
    "                   cv = 5, \n",
    "                   verbose = 2, \n",
    "                   n_jobs= 4)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitted the param into two reduce the time\n",
    "parameters = {\n",
    "    \"loss\":['ls'],\n",
    "    \"learning_rate\": [0.2],\n",
    "    \"min_samples_split\": np.linspace(0.1, 0.5, 5),\n",
    "    \"min_samples_leaf\": np.linspace(0.1, 0.5, 5),\n",
    "    \"max_depth\":[8],\n",
    "    \"max_features\":[\"log2\"],\n",
    "    \"criterion\": [\"friedman_mse\"],\n",
    "    \"subsample\":[0.9],\n",
    "    \"n_estimators\":[10,100]\n",
    "    }\n",
    "clf = GridSearchCV(estimator = GradientBoostingRegressor(), \n",
    "                   param_grid = parameters, \n",
    "                   cv = 5, \n",
    "                   verbose = 2, \n",
    "                   n_jobs= 4)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "model=GradientBoostingRegressor(criterion= 'mae', \n",
    "                                learning_rate= 0.2,\n",
    "                                loss= 'huber',\n",
    "                                max_depth= 5,\n",
    "                                max_features= 'sqrt',\n",
    "                                n_estimators= 100,\n",
    "                                subsample= 0.9,\n",
    "                                min_samples_leaf=0.1,\n",
    "                                min_samples_split= 0.2,)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model=GradientBoostingRegressor(criterion= 'friedman_mse', \n",
    "                                learning_rate= 0.2,\n",
    "                                loss= 'ls',\n",
    "                                max_depth= 8,\n",
    "                                max_features= 'log2',\n",
    "                                n_estimators= 100,\n",
    "                                subsample= 0.9,\n",
    "                                min_samples_leaf=0.1,\n",
    "                                min_samples_split= 0.2,)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "# Train data accuracy\n",
    "print('Performance on training data using DT:',model.score(X_train,y_train))\n",
    "# test data accuracy\n",
    "print('Performance on testing data using DT:',model.score(X_test,y_test))\n",
    "#r2 score\n",
    "acc_DT=metrics.r2_score(y_test, y_pred)\n",
    "print('Accuracy Test: ',acc_DT)\n",
    "print('MSE: ',metrics.mean_squared_error(y_test, y_pred))\n",
    "\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
    "#kfold = KFold(n_splits=num_folds)\n",
    "#model1 = model\n",
    "results = cross_val_score(model, X_train, y_train, cv=kfold)\n",
    "print(results)\n",
    "print(\"\\n Average model Accuracy: %.3f%% with std. dev - (%.3f%%)\" % (results.mean()*100.0, results.std()*100.0))\n",
    "\n",
    "modelComp=modelComp.append(pd.DataFrame({'Model':['GradientBoostingRegressor'],\n",
    "                                         'Train Accuracy':[model.score(X_train,y_train)],\n",
    "                                         'Test Accuracy':[model.score(X_test , y_test)],\n",
    "                                         'Kfold-Mean-Accuracy':[results.mean()],\n",
    "                                         'Kfold-StdDeviation':[results.std()]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Traing and test data sets accuracies are very close around 90% and the cross val score is as well close to 85%\n",
    "- This model is more geenralised as the train and test accuracies are very close at 92% and 89% respectively\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelComp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### AdaBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=AdaBoostRegressor()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred = model.predict(X_test)\n",
    "# Train data accuracy\n",
    "print('Performance on training data using DT:',model.score(X_train,y_train))\n",
    "# test data accuracy\n",
    "print('Performance on testing data using DT:',model.score(X_test,y_test))\n",
    "#r2 score\n",
    "acc_DT=metrics.r2_score(y_test, y_pred)\n",
    "print('Accuracy Test: ',acc_DT)\n",
    "print('MSE: ',metrics.mean_squared_error(y_test, y_pred))\n",
    "\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
    "#kfold = KFold(n_splits=num_folds)\n",
    "model1 = model\n",
    "results = cross_val_score(model1, X_train, y_train, cv=kfold)\n",
    "print(results)\n",
    "print(\"\\n Average model Accuracy: %.3f%% with std. dev - (%.3f%%)\" % (results.mean()*100.0, results.std()*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom prameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=AdaBoostRegressor(n_estimators=100,\n",
    "    learning_rate=1,\n",
    "    loss='linear')\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred = model.predict(X_test)\n",
    "# Train data accuracy\n",
    "print('Performance on training data using DT:',model.score(X_train,y_train))\n",
    "# test data accuracy\n",
    "print('Performance on testing data using DT:',model.score(X_test,y_test))\n",
    "#r2 score\n",
    "acc_DT=metrics.r2_score(y_test, y_pred)\n",
    "print('Accuracy Test: ',acc_DT)\n",
    "print('MSE: ',metrics.mean_squared_error(y_test, y_pred))\n",
    "\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
    "#kfold = KFold(n_splits=num_folds)\n",
    "model1 = model\n",
    "results = cross_val_score(model1, X_train, y_train, cv=kfold)\n",
    "print(results)\n",
    "print(\"\\n Average model Accuracy: %.3f%% with std. dev - (%.3f%%)\" % (results.mean()*100.0, results.std()*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- AdaBoost Regressor Traing and Test set accuracies are around 80%, so the model is a not a over fit or under fit model.\n",
    "- But the cross val score is low compared to other regreesors\n",
    "- We were able to sqeeze out little extra gain in both train and test accuracies but its still not comparable to other regressors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelComp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Comparing these selective details, we can notice the train and test accuracies are closest for gradient boost followed by Random forest regressor\n",
    "- Keeping in mind about overfitting modles we are tilting towrds GBM it has showed similar test accuracy and train accuracies were also close by.\n",
    "- So far we can consider Gradient boost as its been more generalized and its Train/Test/cross-val accuracies are nearby compared to other models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bootstrap Sampling and Confidence Interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import accuracy_score\n",
    "from matplotlib import pyplot\n",
    "data = X.join(y)\n",
    "values = data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "# Number of bootstrap samples to create\n",
    "n_iterations = 1000        \n",
    "# size of a bootstrap sample\n",
    "n_size = int(len(data) * 0.5)    \n",
    "\n",
    "# run bootstrap\n",
    "# empty list that will hold the scores for each bootstrap iteration\n",
    "stats = list()   \n",
    "for i in range(n_iterations):\n",
    "    # prepare train and test sets\n",
    "    train = resample(values, n_samples=n_size)  # Sampling with replacement \n",
    "    test = np.array([x for x in values if x.tolist() not in train.tolist()])  # picking rest of the data not considered in sample\n",
    "    \n",
    "    \n",
    "     # fit model\n",
    "    model = RandomForestRegressor(\n",
    "        bootstrap = True,\n",
    "        max_depth= 8,\n",
    "        max_features= 'sqrt',\n",
    "        min_samples_leaf= 1,\n",
    "        n_estimators= 100)\n",
    "    # fit against independent variables and corresponding target values\n",
    "    model.fit(train[:,:-1], train[:,-1]) \n",
    "    # Take the target column for all rows in test set\n",
    "\n",
    "    y_test = test[:,-1]    \n",
    "    # evaluate model\n",
    "    # predict based on independent variables in the test data\n",
    "    score = model.score(test[:, :-1] , y_test)\n",
    "    predictions = model.predict(test[:, :-1])  \n",
    "\n",
    "    stats.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot scores\n",
    "pyplot.hist(stats)\n",
    "pyplot.show()\n",
    "# confidence intervals\n",
    "alpha = 0.95                             # for 95% confidence \n",
    "p = ((1.0-alpha)/2.0) * 100              # tail regions on right and left .25 on each side indicated by P value (border)\n",
    "lower = max(0.0, np.percentile(stats, p))  \n",
    "p = (alpha+((1.0-alpha)/2.0)) * 100\n",
    "upper = min(1.0, np.percentile(stats, p))\n",
    "print('%.1f confidence interval %.1f%% and %.1f%%' % (alpha*100, lower*100, upper*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Boost\n",
    "# Number of bootstrap samples to create\n",
    "n_iterations = 1000        \n",
    "# size of a bootstrap sample\n",
    "n_size = int(len(data) * 0.5)    \n",
    "\n",
    "# run bootstrap\n",
    "# empty list that will hold the scores for each bootstrap iteration\n",
    "stats = list()   \n",
    "for i in range(n_iterations):\n",
    "    # prepare train and test sets\n",
    "    train = resample(values, n_samples=n_size)  # Sampling with replacement \n",
    "    test = np.array([x for x in values if x.tolist() not in train.tolist()])  # picking rest of the data not considered in sample\n",
    "    \n",
    "    \n",
    "     # fit model\n",
    "        \n",
    "    model = GradientBoostingRegressor(criterion= 'mae', \n",
    "                                learning_rate= 0.2,\n",
    "                                loss= 'huber',\n",
    "                                max_depth= 5,\n",
    "                                max_features= 'sqrt',\n",
    "                                n_estimators= 100,\n",
    "                                subsample= 0.9,\n",
    "                                min_samples_leaf=0.1,\n",
    "                                min_samples_split= 0.2,)\n",
    "    # fit against independent variables and corresponding target values\n",
    "    model.fit(train[:,:-1], train[:,-1]) \n",
    "    # Take the target column for all rows in test set\n",
    "\n",
    "    y_test = test[:,-1]    \n",
    "    # evaluate model\n",
    "    # predict based on independent variables in the test data\n",
    "    score = model.score(test[:, :-1] , y_test)\n",
    "    predictions = model.predict(test[:, :-1])  \n",
    "\n",
    "    stats.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot scores\n",
    "\n",
    "pyplot.hist(stats)\n",
    "pyplot.show()\n",
    "# confidence intervals\n",
    "alpha = 0.95                             # for 95% confidence \n",
    "p = ((1.0-alpha)/2.0) * 100              # tail regions on right and left .25 on each side indicated by P value (border)\n",
    "lower = max(0.0, np.percentile(stats, p))  \n",
    "p = (alpha+((1.0-alpha)/2.0)) * 100\n",
    "upper = min(1.0, np.percentile(stats, p))\n",
    "print('%.1f confidence interval %.1f%% and %.1f%%' % (alpha*100, lower*100, upper*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- For Gradient Boost - 95.0 confidence interval 82.6% and 87.5%\n",
    "- For Random Forest - 95.0 confidence interval 81.6% and 86.6%\n",
    "- there is s slight advantage for Gradient Boost model compared to Random Forest\n",
    "- Comparing the model performance range at 95% confidence we can notice its a tough decission to decide between the models solely comparing the number. We have to weigh in the model design and what need to be considered for the data and complexity.\n",
    "- RF basically has only one hyperparameter to set: the number of features to randomly select at each node. However there is a rule-of-thumb to use the square root of the number of total features which works pretty well in most cases but need to look upon case by case. On the other hand, GBMs have several hyperparameters that include the number of trees, the depth (or number of leaves), and the shrinkage (or learning rate).\n",
    "- There is one fundamental difference in performance between the two that may force you to choose Random Forests over Gradient Boosted Machines (GBMs). That is, Random Forests can be easily deployed in a distributed fashion due to the fact that they can run in parallel, whereas Gradient Boosted Machines only run trial after trial.\n",
    "- Considering our scenario we have limited set of data hence the the cons for parallelism is not affecting us heavily. Along with that our data has very less linear relations and GBM approach of multiple iteration for adding weight might give us added advantage over RF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Reasons for choosing GBM\n",
    "- RF overfit a sample of the training data and then reduces the overfit by simple averaging the predictors. But GBM repeatedly train trees or the residuals of the previous predictors.\n",
    "- RF is easy to use (less tuning parameters). we can blindly apply RF and can get decent performance with a little chance of overfit but without cross validation GBM is useless. GBM need much care to setup. As in GM we can tune the hyperparameters like no of trees, depth, learning rate so the prediction and performance is better than the Random forest.\n",
    "- With continuos learning and exposure to data we can fine tune GBM more accurately and in generalized form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
